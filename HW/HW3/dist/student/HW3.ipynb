{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e3a1c8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"HW3.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54fa7b2-699d-42e0-b017-ebb285b0f9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a581dc-c9ca-4cc5-87b9-06a3ea7dd709",
   "metadata": {},
   "source": [
    "# SMS Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd10d30e-96a1-446e-b576-33ed5da41491",
   "metadata": {},
   "source": [
    "We often receive lots of spam texts on our phone. You will design a logistic regression model which can be incorporated later in a phone app to filter out all such spam messages.\n",
    "\n",
    "# Task 1: Preprocessing-I\n",
    "\n",
    "You will be training an svm model to classify whether a message is `spam` (label = 1) or `ham` (label = 0). In particular, you need to convert each message into a feature vector $x \\in R^n$. But before you perform such conversion, you need to preprocess the text data first.\n",
    "\n",
    "You will perform the following preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa1567-4a6d-4a83-90eb-2346e8e81505",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1(a)\n",
    "\n",
    "Convert all message into lowercase so that capitalization is ignored. (Eg. `HelLo` and `hello` and `Hello` are all treated the same).\n",
    "\n",
    "Hint: See `np.char.lower()` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3a7da1-a275-4cd0-b1ef-9ba34ec89794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def toLower(X):\n",
    "    \"\"\"\n",
    "    X: a numpy array where each item is a text\n",
    "\n",
    "    Returns: a numpy array where each text is converted in lower case. \n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f588f2a8-f3e6-44d9-8ec4-8bee0cc79f73",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "text = np.array([\"I HAVE A DATE ON SUNDAY WITH WILL!!\"])\n",
    "toLower(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f03fe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d843dbd-f4ef-4b8f-a72a-e9dc408b7e3e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1(b)\n",
    "\n",
    "(i) Replace all two digits followed by a plus symbol as ` explicitContent `. (eg. 18+ should be converted to `explicitContent`). \n",
    "\n",
    "(ii) Likewise, replace all numbers as text `number`. You are expected to find many spam messages containing some phone numbers to call or text. Although\n",
    "most of the texts would contain different type of numbers, however, they all represent same type\n",
    "of entities, that is, calling or texting to a random number. Therefore, one method often employed\n",
    "is to normalize these values, so that all numbers are treated the same. \n",
    "\n",
    "(eg. \"Get txting and tell ur mates. www.getzed.co.uk POBox 36504 W45WQ 16+ norm150p/tone\" should be converted to `Get txting and tell ur mates. www.getzed.co.uk POBox number W number WQ explicitContent norm number p/tone`)\n",
    "\n",
    "Note the extra space around the ` explicitContent ` and ` number `\n",
    "\n",
    "Hint: Look into `np.vectorize()` function to apply a function to each element of a numpy array. Alternatively, you could use `for` loop. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe3a38a-3ab9-4503-8d68-09679b8f8884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalizeDigits(X):\n",
    "    \"\"\"\n",
    "    X: a numpy array where each item is a text\n",
    "\n",
    "    Returns: normalized_X, a numpy array where each text is normalized as per the question. \n",
    "    \"\"\"\n",
    "    normalized_X = np.vectorize(lambda x: re.sub(... , ... , x))(X)\n",
    "    return normalized_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08544b83-5af3-4534-a278-d0db39c7ca8b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "text = np.array([\"Get txting and tell ur mates. www.getzed.co.uk POBox 36504 W45WQ 16+ norm150p/tone\"])\n",
    "normalizeDigits(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a076d0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d4c59-e304-471b-9cf4-69e139ca96a1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1(c)\n",
    "\n",
    "You will find in the text that lots of spam messages are mentioning for certain amount of money as a trap, in dollars and pounds. Your task is to replace\n",
    "all of such symbols with ` dollars `.\n",
    "\n",
    "(eg. `Farm tour 9am to 5pm $95/pax, $50 deposit by 16 May` is converted to `Farm tour 9am to 5pm   dollars 95/pax,  dollars 50 deposit by 16 May`)\n",
    "\n",
    "Note the extra space around the word ` dollars `."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d69f14-2547-4307-8e27-8f9933afdde5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalizeCurrency(X):\n",
    "    \"\"\"\n",
    "    X: a numpy array where each item is a text\n",
    "\n",
    "    Returns: normalized_X, a numpy array where each text is normalized as per the question. \n",
    "    \"\"\"\n",
    "    normalized_X = ...\n",
    "    return normalized_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791673ed-3c51-4554-9c22-db0c57309455",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "text = np.array([\"Farm tour 9am to 5pm $95/pax, $50 deposit by 16 May\"])\n",
    "normalizeCurrency(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6ef04",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1(c)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d90952-7cd5-4510-8f36-0f9c2880f758",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1(d)\n",
    "\n",
    "Strip all(or most of) URL content and email addresses from the messages and replace it with ` addr `. \n",
    "\n",
    "Note the extra space around the word ` addr `. \n",
    "The regex to extract them is provided to you. \n",
    "\n",
    "eg. \"XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL\" will be converted to `XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap.  urladdr `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8208a87-19bb-41e0-88ec-27841dcc88d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalizeAddress(X):\n",
    "    \"\"\"\n",
    "    X: a numpy array where each item is a text\n",
    "\n",
    "    Returns: normalized_X, a numpy array where each text is normalized as per the question. \n",
    "    \"\"\"\n",
    "    \n",
    "    regex = r\"(?:http[s]?:\\/\\/.)?(?:www\\.)?[-a-zA-Z0-9@%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b(?:[-a-zA-Z0-9@:%_\\+.~#?&\\/\\/=]*)\" \n",
    "\n",
    "    normalized_X = ...\n",
    "    \n",
    "    return normalized_X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ccbe1-de10-48c8-a15c-614c49768e81",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "text = np.array([\"XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL\"])\n",
    "normalizeAddress(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b88a2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f668b4cb-5a3a-47c1-b072-0a1ba674bad0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1(e)\n",
    "\n",
    "(i) Remove all non-alphanumeric words, white spaces (tabs, newlines, extra spaces) and punctuations from the text.\n",
    "\n",
    "(ii) Remove all single character words from the text. \n",
    "\n",
    "(iii) Remove all the stop words. Stop words are the most frequent words, such as ‘an’, ‘the’, ‘for’ etc, which contain very little information about the text content itself.\n",
    "\n",
    "\n",
    "Eg: `A gram usually runs like     &lt;#&gt; , a half eighth is smarter though and gets you almost a whole second gram for  &lt;#&gt;` will be converted to `gram usually runs like ltgt half eighth smarter though gets almost whole second gram ltgt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75e7401-0232-4393-b68c-1806a100bded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41848ab5-7e77-4219-bca0-a78588452d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_stopwords = stopwords.words('english')\n",
    "my_stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa6cdb-f107-45a9-ad9d-14b566f68b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def removeStopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in my_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cec8c3-adfa-4d4c-850d-116116d22b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanText(X):\n",
    "    \"\"\"\n",
    "    X: a numpy array where each item is a text\n",
    "\n",
    "    Returns: cleaned_X, a numpy array where each text is normalized as per the question.\n",
    "    \"\"\"\n",
    "    cleaned_X = ...\n",
    "    return cleaned_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15098374-9bb8-4aaf-ae65-4353fd17d984",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "text = np.array([\"A gram usually runs like     &lt;#&gt; , a half eighth is smarter though and gets you almost a whole second gram for  &lt;#&gt;\"])\n",
    "cleanText(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3299e423",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc464b4-765e-42ee-8896-9f80470b9d39",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1(f)\n",
    "\n",
    "Words are reduced to their stemmed form. For ex: ‘wait’, ‘waits’,\n",
    "‘waited’, ‘waiting’ are all replaced with ‘wait’. Sometimes, the Stemmer actually strips off additional\n",
    "1\n",
    "characters from the end, so ‘include’, ‘includes’, ‘included’ and ‘including’ are all replaced with\n",
    "‘includ’.\n",
    "After preprocessing, the first message in the dataset will look like list of the following words: `go until jurong point, crazy.. avail onli in bugi n great world la e buffet... cine there got amor wat...`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70cc29-1b18-4a06-b8cb-bf59042d510f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba048df-1a86-4271-898e-42c7bdee7fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stemwords(text):\n",
    "    stemmed_text = ...\n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f672278-435d-487d-8daf-23eb36356baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stemTexts(X):\n",
    "    return np.vectorize(stemwords)(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8ddfe-3c6b-4c5c-bdaa-6d768fdcfe8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = np.array([\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\"])\n",
    "stemTexts(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0fbe5f-22ef-4717-894b-ab45efdfeef0",
   "metadata": {},
   "source": [
    "Lets now process our dataset through the preprocessing model we implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3197b75-2301-40f1-82c2-da611b1a9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd9306b-7797-4fe1-ae7b-7529f98e7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "texts = []\n",
    "\n",
    "with open('data/SMSSpamCollection') as file:\n",
    "    for line in file:\n",
    "        my_list = line.split()\n",
    "        if(my_list[0] == \"spam\"):\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "        texts.append((' ').join(my_list[1:]))\n",
    "\n",
    "Y = np.array(labels)\n",
    "X = np.array(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773d437-f840-498a-8174-dc31cbf27050",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c498bec-d4ed-4d56-892b-b2264740252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5328c17-e816-4139-ba71-86983deed157",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e3d7c-e017-45d0-b81a-8770d2c1c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_1 = Pipeline([\n",
    "                            ('1a', FunctionTransformer(toLower)),\n",
    "                            ('1b', FunctionTransformer(normalizeDigits)),\n",
    "                            ('1c', FunctionTransformer(normalizeCurrency)), \n",
    "                            ('1d', FunctionTransformer(normalizeAddress)), \n",
    "                            ('1e', FunctionTransformer(cleanText)), \n",
    "                            ('1f', FunctionTransformer(stemTexts)), \n",
    "                          ])\n",
    "preprocessed_1_X = preprocessing_1.transform(X)\n",
    "preprocessed_1_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec0f5e-26b3-4e28-aec0-330bd3701911",
   "metadata": {},
   "source": [
    "# Task 2: Preprocessing-II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7148b5b9-34ed-4bb2-abda-746421fcbf2e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2(a)\n",
    "\n",
    "After preprocessing the dataset, we have a list of words for each message. The next step is to\n",
    "choose which words we would like to use in our classifier and which we want to leave out.\n",
    "For this task, you will chose only the most frequently occuring words in the overall dataset as our\n",
    "vocabulary list. Since word that occur rarely in the training set are only in a few messages, they\n",
    "might cause the model to overfit our training set.\n",
    "I ran the model by chosing words which occur at least 100 times in the spam dataset. You can\n",
    "either handpick this number or tune it as a hyperparameter. It will result into a vocabulary list `vocab_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaab0b8-3bf1-416c-acb2-348b20e673f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "length_vocablist = 100 # you can tune this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d1c84-dcf9-428d-b1bd-4113ff86bfd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Text_to_Vec(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n = 100):\n",
    "        self.n = n\n",
    "        self.vocab_list = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        X: a numpy array, where each item is a text\n",
    "        \n",
    "        Returns: vocab_list, a list containing top n frequent words in X\n",
    "        \"\"\"\n",
    "        \n",
    "        self.vocab_list = ...\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ef070-695b-4eb6-82cd-475a04427aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word2vec = Text_to_Vec()\n",
    "word2vec.fit(preprocessed_1_X)\n",
    "vocab_list = word2vec.vocab_list\n",
    "vocab_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc699c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04f23e-9d95-401d-8aab-82c55c41e1f4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2(b)\n",
    "\n",
    "Give the vocabulary list, we can now map each word in the preprocessed messages into a list of word\n",
    "indices that contains the index of the word in the vocabulary list. For eg: let’s say your vocabulary\n",
    "list is [‘world’, ‘amazing’, ‘nice’, ‘apple’, ‘hello’], then the word indices for the text message ‘hello\n",
    "world’ will be [4, 0], since hello is at index 4 and world at index 0 in the message.\n",
    "\n",
    "You will now implement the feature extraction that converts each text message into a vector in\n",
    "$x \\in R^n$.\n",
    "For this task, you will be using n = number of words in vocabulary list. Specifically, the feature $x_i \\in {0, 1}$\n",
    "for a message corresponds to whether the $i^{th}$ word in the vocabulary list occurs in the email. That\n",
    "is, $x_i = 1$ if the $i^{th}$ word is in the message, and otherwise 0.\n",
    "For eg. `hello world` will be represented as [1 0 0 0 1]. where the size of each text message is the\n",
    "length of the vocabulary list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560db75f-2de5-4600-865c-8076808c8e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Text_to_Vec(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n = 100):\n",
    "        self.n = n\n",
    "        self.vocab_list = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        X: a numpy array, where each item is a text\n",
    "        \n",
    "        Returns: vocab_list, a list containing top n frequent words in X\n",
    "        \"\"\"\n",
    "        \n",
    "        self.vocab_list = ...\n",
    "        return self\n",
    "\n",
    "    def one_hot(self, text):\n",
    "        \"\"\"\n",
    "        text: a string, takes a message as input\n",
    "        \n",
    "        Returns: one_hot_encoder, a list containing the index number of the word in the vocabulary list for each word in the message\n",
    "        Eg: for the message \"Hello World\" it will return [4 0] as per the example shown in the question\n",
    "        \"\"\"\n",
    "        one_hot_encoder = []\n",
    "        ...\n",
    "        return one_hot_encoder\n",
    "\n",
    "    def vectorize(self, text):\n",
    "        \"\"\"\n",
    "        text: a string, takes a message as input\n",
    "        \n",
    "        Returns: vec, a numpy array containing the one-hot-encoding of the text. Eg: for the message \"Hello World\" it will return [1 0 0 0 1]\n",
    "        \"\"\"\n",
    "        vec = np.zeros((self.n,))\n",
    "        one_hot_encoder = self.one_hot(text)\n",
    "        ...\n",
    "        return vec\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        X: a numpy array, where each element is a text\n",
    "\n",
    "        Returns: vectorized_X, a numpy array where each element is the one-hot encoding of the element/text.\n",
    "        \"\"\"\n",
    "        vectorized_X = np.zeros((len(X), self.n))\n",
    "        ...\n",
    "        return vectorized_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579f34f-e62a-4f74-9e5f-7738d416a618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "pipe = Pipeline([('1', preprocessing_1), ('2', Text_to_Vec())])\n",
    "X = pipe.fit_transform(preprocessed_1_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638dbcbc-897d-4d42-9692-4fe5bbf68ffb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a0112-6285-4ebf-9f37-e9c8b7fe8d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641078d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464df57b-a5c8-4908-b93f-6b34b03097b3",
   "metadata": {},
   "source": [
    "# Task 3: Implement Logistic Regression Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62520b0-3cb2-4ff7-8b5e-38d3b375df03",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3a\n",
    "Divide your data into 80-20 split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c356a-49b0-4eef-9851-ee59a3933cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(X, Y):\n",
    "    train_X, test_X = ...\n",
    "    train_Y, test_Y = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473313b2-eee6-4b9d-ab5f-ec478225cc40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_X.shape, train_Y.shape, test_X.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6836ad68",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c9bb0-1aaa-4e37-b0cf-80c74f5db96a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3(b)\n",
    "\n",
    "Once you have extracted the feature vector in Task 2, that is, convert each text into vector, train a logistic regression model to classify between the training examples. You will perform GridSearch and k-fold cross validation technique on the training dataset to tune the model hyperparameter `lr` and `lambda` for L2 regularization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadde579-b630-4310-8493-56517a9bdc32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lr = 0.01, batch_size = 32, n_epoch = 50, reg = 0.1):\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epoch = n_epoch\n",
    "        self.theta = None\n",
    "        self.reg = reg\n",
    "\n",
    "    def add_1(self, X):\n",
    "        \"\"\"\n",
    "        X: a numpy array of shape (m, n)\n",
    "        returns: a new array X, with shape (m, n+1) where the first column is a column vector of ones\n",
    "        \"\"\"\n",
    "        intercept = ...\n",
    "        ...\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        \"\"\"\n",
    "        X: a numpy array of shape (m, n+1)\n",
    "        returns: a new array where each element is transformed using sigmoid activation function\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def hypothesis(self, X, theta):\n",
    "        \"\"\"\n",
    "        X: a numpy array of shape (m, n+1)\n",
    "        theta: a numpy array of shape (n+1, 1)\n",
    "\n",
    "        return: y_hat, a numpy array of shape (m, 1) where values are between 0 and 1\n",
    "        \"\"\"\n",
    "        #print(X.shape, theta.shape)\n",
    "        y_hat = ...\n",
    "        return y_hat\n",
    "\n",
    "    def gradient(self, X, y, theta):\n",
    "        \"\"\"\n",
    "        X: a numpy array of shape (m, n+1)\n",
    "        y: a numpy array of shape (m, 1)\n",
    "        theta: a numpy array of shape (n+1, 1)\n",
    "\n",
    "        returns: gradient vector (derivative of loss function wrt theta), a numpy array of shape (n+1, 1)\n",
    "\n",
    "        Hint: Use backpropagation to implement it; Don't forget to add the gradient because of the l2-regularization\n",
    "        \"\"\"\n",
    "        y_hat = self.hypothesis(X, theta)\n",
    "        grad = ...\n",
    "        l2_grad = ...\n",
    "        return grad + l2_grad\n",
    "\n",
    "    def cost(self, X, y, theta):\n",
    "        \"\"\"\n",
    "        X: a numpy array of shape (m, n+1)\n",
    "        y: a numpy array of shape (m, 1)\n",
    "        theta: a numpy array of shape (n+1, 1)\n",
    "\n",
    "        returns: rmse (a scalar) \n",
    "\n",
    "        Hint: Use Cross-entropy loss function; don't forget to add additional cost (l2_cost) due to L2 regularization\n",
    "        \"\"\"\n",
    "        y_hat = self.hypothesis(X, theta)\n",
    "        cost = ...\n",
    "        l2_cost = ...\n",
    "        return cost + l2_cost\n",
    "\n",
    "    def gradient_descent(self, X, y, theta):\n",
    "        \"\"\"\n",
    "        X: a numpy array of shape (m, n+1)\n",
    "        y: a numpy array of shape (m,1)\n",
    "        theta: a numpy array of shape (n+1, 1)\n",
    "\n",
    "        returns: updated theta according to the gradient descent algorithm\n",
    "        \"\"\"\n",
    "        theta = ...\n",
    "        return theta\n",
    "        \n",
    "\n",
    "    def fit(self, trainX = None, trainY = None):\n",
    "        m, n = trainX.shape\n",
    "        self.theta = np.zeros((n+1,1))\n",
    "\n",
    "        # add a column vector of 1's in X for the intercept/bias term\n",
    "        trainX = self.add_1(trainX)\n",
    "\n",
    "        trainY = np.expand_dims(trainY, axis=1)\n",
    "\n",
    "        n_splits = math.ceil(m/self.batch_size)\n",
    "\n",
    "        for epoch in np.arange(self.n_epoch):\n",
    "            ixs = np.arange(m)\n",
    "            np.random.shuffle(ixs)\n",
    "            for batch_num, batch_ixs in enumerate(np.array_split(ixs, n_splits)):\n",
    "                self.theta = self.gradient_descent(trainX[batch_ixs], trainY[batch_ixs], self.theta)\n",
    "\n",
    "            training_error = self.cost(trainX, trainY, self.theta)\n",
    "            #print(f\"Epoch: {epoch}; Training Error: {training_error}\")\n",
    "                  \n",
    "    def predict(self, X, y=None, threshold = 0.5):\n",
    "        \"\"\"\n",
    "        X: a numpy array of shape (m, n)\n",
    "\n",
    "        Returns: a numpy array of shape (m, 1) where each element is a value 0 (threshold <= 0.5) or 1 (threshold > 0.5)\n",
    "        \"\"\"\n",
    "        X = self.add_1(X)\n",
    "        y_hat = ...\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e7b7f-f746-4299-91e0-b663d59743e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_model = LogisticRegressionModel(lr = 0.001, batch_size = 128, n_epoch = 5000, reg = 0.1)\n",
    "\n",
    "my_model.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030bb526-26a1-4fd8-8be8-c3e3ab7d0f37",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3(c)\n",
    "\n",
    "Implement 5-fold cross validation to tune the hyperparameters using GridSearch. \n",
    "\n",
    "State the hyperparameter values for which you achieved maximum accuracy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b70e7-3b9e-41f5-bafa-7503ae1c40bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_validate(model, X, y, n_splits=5):\n",
    "    \"\"\"\n",
    "    X: a numpy array of shape (m, n)\n",
    "    y: a numpy array of shape (m, )\n",
    "    \"\"\"\n",
    "    data = np.c_[X, y]\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    folds = np.array_split(data, n_splits)\n",
    "\n",
    "    fold_accuracies = [] # a list containing the accuracy for each fold\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        val_fold = ...\n",
    "        train_folds = ...\n",
    "\n",
    "    X_train = train_folds[:, :-1] \n",
    "    y_train = train_folds[:, -1] \n",
    "    X_val = val_fold[:, :-1] \n",
    "    y_val = val_fold[:, -1] \n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    accuracy = np.mean(y_pred == y_val)\n",
    "    fold_accuracies.append(accuracy)\n",
    "\n",
    "    return np.mean(fold_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e87742-abbb-4f82-ab38-a5e47c9b6926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_X.shape, train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba2d3ca-bd81-4928-8b45-1decbec3fd63",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell will take time to execute since you are training it for each combination of hyperparameters\n",
    "history = []\n",
    "for lambda_reg in [10**-3, 10**-2, 10**-1, 1, 10]:\n",
    "    for lr in [10**-4, 30**-4, 10**-3, 30**-3, 10**-2, 30**-2, 10**-1, 30**-1, 1]:\n",
    "        my_dict = {}\n",
    "        my_dict['lambda'] = lambda_reg\n",
    "        my_dict['lr'] = lr\n",
    "        my_model = LogisticRegressionModel(lr = lr, batch_size = 128, n_epoch = 5000, reg = lambda_reg)\n",
    "        my_dict['acc'] = cross_validate(my_model, train_X, train_Y)\n",
    "        history.append(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036f531-b4f2-407b-b39c-3c0c17ae7865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e8421e-2207-41ac-b6cd-6e55e08edc6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_final_model = LogisticRegressionModel(lr = ... , batch_size = 128, n_epoch = 5000, reg = ...) # replace it with the best hyperparameter values\n",
    "my_final_model.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90fa60e-1989-4fbe-a482-a3f3ba3780ca",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3d\n",
    "\n",
    "What is the precision and recall of this model?\n",
    "\n",
    "Make sure, you use the test dataset to determine the value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c48c6a-6c99-442b-916b-fe1f1af6ebb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_y = my_final_model.predict(test_X).flatten()\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28eb2f-0253-4975-b9e1-2619bea3518b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "false_positives = ...\n",
    "true_positives = ...\n",
    "\n",
    "precision = ...\n",
    "precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f1f47-7f34-4482-955d-9b4e0180e2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "false_negatives = ...\n",
    "true_positives = ...\n",
    "\n",
    "recall = ...\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270ffe7b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e89bbbe-5eee-486a-98ec-876fdb2a72b3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3e\n",
    "\n",
    "Print the top word predictor which helped in distinguishing a spam message from a ham message. \n",
    "\n",
    "Hint: Find the parameters with the largest values in the classfier and display the corresponding words. Thus, if an email contains words such as `dollars`, `number`, `free` (top predictor words) etc then it is likely to be classified as spam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ee620-5606-4638-a39e-04c43bfb4d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_predictor_word = ...\n",
    "top_predictor_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f50ef",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebcf687-eaf3-4485-bd23-9d29b277f6e5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 3(f): Optional 30 points\n",
    "\n",
    "Since the number of labels are unbalanced, your model will weigh more towards the ham data, and in order to avoid it, you would like to penalize your model more when it misclassifies a spam data into ham, so that, it can't avoid to make such error in future. You can easily do this, by making the loss function increase by a factor of inverse frequency of the occuring labels. That is, \n",
    "\n",
    "(i) First, find the frequency of each label (j)\n",
    "\n",
    "$$N_c = \\frac{1}{M}\\sum^M_1(\\hat{y} == j)$$\n",
    "\n",
    "(ii) Calculate the class weight for label j: \n",
    "\n",
    "$$\\frac{M}{K * N_c}$$\n",
    "\n",
    "This formula ensures that the class weight for each class is inversely proportional to the frequency of that class in the dataset. \n",
    "\n",
    "(iii) Finally, multiply your loss function by class weight computed above for the corresponding label. \n",
    "\n",
    "\n",
    "Note: You would also need to update the gradient function due to change in the loss function. \n",
    "\n",
    "Rewrite the  `cost()` and `grad()` function of Text_to_Vec class to handle class imbalance problem in the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543c8778-8395-480e-90be-68f27a9323ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.unique(train_Y, return_counts=True) # Class imbalance Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ca701-83da-4977-9ecf-b12eaf5da54b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1(c)": {
     "name": "q1(c)",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> normalizeCurrency(np.array(['Ur balance is now £500.']))[0] == 'Ur balance is now  dollars 500.'\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1a": {
     "name": "q1a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> toLower(np.array(['PRIVATE! Your 2003 Account Statement for 07753741225 shows 800 un-redeemed S. I. M. points. Call 08715203677 Identifier Code: 42478 Expires 24/10/04']))[0].islower() == True\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> normalizeDigits(np.array(['call free on 08000407165 (18+) 2 stop getstop on 88222']))[0] == 'call free on  number  ( explicitContent )  number  stop getstop on  number '\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1d": {
     "name": "q1d",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> text = np.array(['Reminder: You have not downloaded the content you have already paid for. Email justing@sfasu.yahoo.com to collect your content.'])\n>>> normalizeAddress(text)\narray(['Reminder: You have not downloaded the content you have already paid for. Email  addr  to collect your content.'],\n      dtype='<U110')",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1e": {
     "name": "q1e",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> cleanText(np.array(['SMS AUCTION - A BRAND NEW Nokia 7250 is up 4 auction today! Auction is FREE       2 join & take part! Txt NOKIA to 86021 now!'])).item() == 'SMS AUCTION BRAND NEW Nokia 7250 4 auction today Auction FREE 2 join take part Txt NOKIA 86021'\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(vocab_list) == length_vocablist\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(X) == len(Y)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> train_X.shape[0] == train_Y.shape[0] and (train_X.shape[0] + test_X.shape[0] == X.shape[0]) == True\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3d": {
     "name": "q3d",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (0 <= precision <= 100).item() == True\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (0 <= recall <= 100).item() == True\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3e": {
     "name": "q3e",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> top_predictor_word in vocab_list\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
